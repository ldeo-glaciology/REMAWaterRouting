{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-possession",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "simple-emergency",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-b311f7ad264f>:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask\n",
    "import dask.delayed\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from rasterio import RasterioIOError\n",
    "from tqdm.autonotebook import tqdm\n",
    "%matplotlib inline\n",
    "import sklearn.linear_model\n",
    "import skimage.morphology\n",
    "import skimage.segmentation\n",
    "import skimage.future\n",
    "#import richdem as rd\n",
    "import scipy.ndimage\n",
    "import dask\n",
    "import math\n",
    "import scipy.sparse\n",
    "import shapefile\n",
    "import shapely\n",
    "import rioxarray\n",
    "import cartopy as crt\n",
    "from shapely.geometry import mapping\n",
    "#import networkx as nx\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "\n",
    "#import cv2\n",
    "import gcsfs\n",
    "import json\n",
    "import h5py\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "brilliant-medium",
   "metadata": {},
   "source": [
    "pip install https://github.com/chunglabmit/pystripe/archive/master.zip"
   ]
  },
  {
   "cell_type": "raw",
   "id": "substantial-pressure",
   "metadata": {},
   "source": [
    "pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "raw",
   "id": "atmospheric-novel",
   "metadata": {},
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "raw",
   "id": "governmental-canvas",
   "metadata": {},
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "raw",
   "id": "above-electric",
   "metadata": {},
   "source": [
    "cd "
   ]
  },
  {
   "cell_type": "raw",
   "id": "statistical-shanghai",
   "metadata": {},
   "source": [
    "cd REMAWaterRouting/Functions/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ongoing-artwork",
   "metadata": {},
   "source": [
    "#import HydrologyFunctions as hf\n",
    "\n",
    "from Destriping_functions import Lloyd_destripe, Guan_destripe, Rogass_destripe,\\\n",
    "Pande_Chhetri_destripe, Pystripe_destripe, RMSE_metric, SSIM_metric, PSNR_metric\n",
    "from Oblique_destriping_functions import striping_angle, grad_image,\\\n",
    "super_Gauss_filter, refinement_destriping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "numeric-participant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\n"
     ]
    }
   ],
   "source": [
    "cd  .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "favorite-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask\n",
    "from dask_gateway import Gateway\n",
    "from dask.distributed import Client\n",
    "import dask_gateway\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-newsletter",
   "metadata": {},
   "source": [
    "## Open Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "current-adapter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ClusterReport<name=prod.c3caa6bb6cf34c9cbdacd075579634dd, status=RUNNING>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the dask-gateway version\n",
    "dask_gateway.__version__\n",
    "\n",
    "# show the default dask-gateway settings\n",
    "dask.config.config['gateway']\n",
    "\n",
    "# show the current default image to be started on workers\n",
    "os.environ['JUPYTER_IMAGE_SPEC']\n",
    "\n",
    "# use the same GDAL environment settings as we did for the single COG case\n",
    "env = dict(GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR', \n",
    "           AWS_NO_SIGN_REQUEST='YES',\n",
    "           GDAL_MAX_RAW_BLOCK_CACHE_SIZE='200000000',\n",
    "           GDAL_SWATH_SIZE='200000000',\n",
    "           VSI_CURL_CACHE_SIZE='200000000')\n",
    "os.environ.update(env)\n",
    "\n",
    "gateway = Gateway()\n",
    "#options = gateway.cluster_options()\n",
    "#options.environment = env \n",
    "#cluster = gateway.new_cluster(options)\n",
    "#cluster.scale(4) # let's get the same number of \"workers\" as our previous LocalCluster examples\n",
    "cluster = gateway.new_cluster()\n",
    "\n",
    "gateway.list_clusters()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adaptive-language",
   "metadata": {},
   "source": [
    "gateway.stop_cluster(gateway.list_clusters()[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "massive-marketplace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ClusterReport<name=prod.c3caa6bb6cf34c9cbdacd075579634dd, status=RUNNING>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gateway.list_clusters()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "nervous-crash",
   "metadata": {},
   "source": [
    "# the dashboard_link property will show the link that can be pasted into the Dask labextension\n",
    "cluster.dashboard_link"
   ]
  },
  {
   "cell_type": "raw",
   "id": "first-spouse",
   "metadata": {},
   "source": [
    "## Scale cluster and connect a client\n",
    "\"cluster\" will show the interactive scaling controls and the dashboard link, which can be pasted into the Dask labextension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "contrary-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect a client\n",
    "# the distributed client is used for running parallel tasks with Dask\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "champion-thinking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>gateway://traefik-gcp-uscentral1b-prod-dask-gateway.prod:80/prod.c3caa6bb6cf34c9cbdacd075579634dd</li>\n",
       "  <li><b>Dashboard: </b><a href='/services/dask-gateway/clusters/prod.c3caa6bb6cf34c9cbdacd075579634dd/status' target='_blank'>/services/dask-gateway/clusters/prod.c3caa6bb6cf34c9cbdacd075579634dd/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tls://10.36.25.28:8786' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "solved-laundry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "departmental-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask.diagnostics import ProgressBar\n",
    "import dask\n",
    "import dask.delayed\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from rasterio import RasterioIOError\n",
    "from tqdm.autonotebook import tqdm\n",
    "%matplotlib inline\n",
    "import sklearn.linear_model\n",
    "import skimage.morphology\n",
    "import skimage.segmentation\n",
    "import skimage.future\n",
    "#import richdem as rd\n",
    "import scipy.ndimage\n",
    "import dask\n",
    "import math\n",
    "import scipy.sparse\n",
    "import shapefile\n",
    "import shapely\n",
    "import rioxarray\n",
    "from shapely.geometry import mapping\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "\n",
    "#import cv2\n",
    "import gcsfs\n",
    "import json\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "absolute-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_shelf = 'LarsenC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "entire-defendant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "continent-binary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-5ffcf5648572>:4: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mapper = gcs.get_mapper(np.str('gs://ldeo-glaciology/REMA/processed/'+ice_shelf))\n"
     ]
    }
   ],
   "source": [
    "with open('ldeo-glaciology-bc97b12df06b.json') as token_file:\n",
    "    token = json.load(token_file)\n",
    "gcs = gcsfs.GCSFileSystem(token=token)\n",
    "mapper = gcs.get_mapper(np.str('gs://ldeo-glaciology/REMA/processed/'+ice_shelf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "other-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hollywood-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = xr.open_zarr(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "buried-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ice_shelf=='Baudouin': #Baudouin and Nivlisen are upside down\n",
    "    processed = processed.apply(np.fliplr)\n",
    "    processed = processed.apply(np.flipud)\n",
    "    processed = processed.apply(np.fliplr)\n",
    "    processed = processed.apply(np.flipud)\n",
    "if ice_shelf=='Nivl': #Baudouin and Nivlisen are upside down\n",
    "    processed = processed.apply(np.fliplr)\n",
    "  #  processed = processed.apply(np.flipud)\n",
    "  #  processed = processed.apply(np.fliplr)\n",
    "  #  processed = processed.apply(np.flipud)\n",
    "if ice_shelf=='Riiser': #Riiser is sideways\n",
    "    processed = processed.transpose()\n",
    "    \n",
    "if ice_shelf=='LarsenC': #LarsenC is sideways\n",
    "    processed = processed.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "contrary-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.attrs.update({'crs':'+init=epsg:3031'})\n",
    "processed.attrs.update({'transform': (32.0, 0.0, 1600000.0, 0.0, -32.0, 800000.0)})\n",
    "processed.attrs.update({'res': (32.0, 32.0)})\n",
    "processed.attrs.update({'is_tiled': 1})\n",
    "processed.attrs.update({'nodatavals': (-9999.0,)})\n",
    "processed.attrs.update({'scales': (1.0,)})\n",
    "processed.attrs.update({'offsets': (0.0,)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "related-instrument",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9217, 9773)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.DB_filled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-mozambique",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "likely-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed =  processed.chunk({'x': 2000,'y':2000})\n",
    "cellsize = processed.res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fixed-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_label = processed.DB_unfilled.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acceptable-shoulder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9217, 9773)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-groove",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.QuadMesh at 0x7f43f898dfa0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_label.coarsen(x=10,y=10,boundary='pad').mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-philip",
   "metadata": {},
   "source": [
    "## Calculate the x,y coordinates of the chunks over which to calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_x = np.int_((np.cumsum(DB_label.chunks[0])))\n",
    "chunks_y = np.int_((np.cumsum(DB_label.chunks[1])))\n",
    "chunks_x = chunks_x[chunks_x<DB_label.shape[0]]\n",
    "chunks_y = chunks_y[chunks_y<DB_label.shape[1]]\n",
    "\n",
    "chunks_x = np.concatenate(([0],chunks_x,[DB_label.shape[0]-1]))\n",
    "chunks_y = np.concatenate(([0],chunks_y,[DB_label.shape[1]-1]))\n",
    "chunks_x, chunks_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks={}\n",
    "counter = 0\n",
    "for j in np.arange(1,len(chunks_y)):\n",
    "    for i in np.arange(1,len(chunks_x)):\n",
    "        chunks[counter] = [chunks_x[i-1],chunks_x[i], chunks_y[j-1],chunks_y[j]]\n",
    "        counter = counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_map = np.zeros(DB_label.shape)\n",
    "for i in np.arange(0,len(chunks)):\n",
    "    x1,x2,y1,y2 = chunks[i]\n",
    "    chunk_map[x1:x2,y1:y2] = i\n",
    "\n",
    "plt.imshow(chunk_map.transpose())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-grant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-fraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dask\n",
    "from dask_gateway import Gateway\n",
    "from dask.distributed import Client\n",
    "import dask_gateway\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-princeton",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-organizer",
   "metadata": {},
   "source": [
    "## If not first time: Import csv of drainage catchment properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe\n",
    "DB_merged_prop = pd.read_csv(str(ice_shelf+'_DB_merged_prop.csv'))\n",
    "\n",
    "DB_merged_prop = DB_merged_prop.drop(columns='Unnamed: 0')\n",
    "\n",
    "#coords_str = DB_merged_prop['coords']\n",
    "\n",
    "#coords_list = [coords.strip('[]').split(',') for coords in coords_str]\n",
    "\n",
    "coords_formatted = {}\n",
    "for index,coord in DB_merged_prop.coords.iteritems():\n",
    "    \n",
    "    coords_formatted[index] = np.int_([x.strip('[ ]') for x in DB_merged_prop['coords'][index].strip('[ ]').split(',')])\n",
    "    \n",
    "\n",
    "DB_merged_prop['coords_reformatted'] = coords_formatted.values()\n",
    "\n",
    "#del coords_str\n",
    "#del coords_list\n",
    "del coords_formatted\n",
    "\n",
    "DB_merged_prop = dask.dataframe.from_pandas(DB_merged_prop,npartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "universal-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label lakes\n",
    "islake = processed.P_all.chunk({'x':-1,'y':-1}).data>0\n",
    "\n",
    "islake_filtered = ~dask.array.bitwise_not(islake).map_blocks(skimage.morphology.remove_small_holes, area_threshold=10)\n",
    "\n",
    "islake_labelled_ = islake_filtered.map_blocks(skimage.measure.label,background=0)\n",
    "\n",
    "islake_labelled = islake_labelled_.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "coordinate-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lake_area(regionmask,intensity):\n",
    "    return np.sum(intensity>0)/np.sum(regionmask)\n",
    "def DB_regionprops2(DB,DB_filled):\n",
    "    table_prop = skimage.measure.regionprops_table(DB,DB_filled,properties = ('label','max_intensity'),extra_properties=(lake_area,))\n",
    "    #pd_prop = pd.DataFrame(table_prop,index=table_prop['label'])\n",
    "    #pd.concat([b,pd_prop])\n",
    "    return table_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "hindu-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_dict = {'label','large_lake_drainage','lake_percent'}\n",
    "label=[0]\n",
    "DB_prop2 = pd.DataFrame(data={},index = label,columns=b_dict)\n",
    "for x1,x2,y1,y2 in chunks.values():\n",
    "    test_prop = dask.array.map_blocks(lambda DB,P_all: DB_regionprops2(DB,P_all),DB_label.data[x1:x2,y1:y2],islake_labelled[x1:x2,y1:y2], dtype=dict)\n",
    "    test_prop = test_prop.compute()\n",
    "    pd_prop = pd.DataFrame(data = {'label': test_prop['label'],'large_lake_drainage': test_prop['max_intensity'], 'lake_percent':test_prop['lake_area']},index=test_prop['label'])\n",
    "    DB_prop2 = DB_prop2.append(pd_prop, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "temporal-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop2 = DB_prop2.groupby('label').agg({'large_lake_drainage':'first', 'lake_percent':'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "confirmed-twist",
   "metadata": {},
   "source": [
    "DB_prop2.to_csv(ice_shelf+'_large_lake_drainage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "precious-advancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_merged_prop = DB_merged_prop.merge(DB_prop2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "worthy-porter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>area</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>catchment_volume</th>\n",
       "      <th>coords</th>\n",
       "      <th>Area_in_m</th>\n",
       "      <th>volume_to_area_ratio</th>\n",
       "      <th>coords_reformatted</th>\n",
       "      <th>large_lake_drainage</th>\n",
       "      <th>lake_percent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: merge, 21 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                label   area eccentricity max_depth catchment_volume  coords Area_in_m volume_to_area_ratio coords_reformatted large_lake_drainage lake_percent\n",
       "npartitions=10                                                                                                                                                 \n",
       "                int64  int64      float64   float64          float64  object   float64              float64             object               int64      float64\n",
       "                  ...    ...          ...       ...              ...     ...       ...                  ...                ...                 ...          ...\n",
       "...               ...    ...          ...       ...              ...     ...       ...                  ...                ...                 ...          ...\n",
       "                  ...    ...          ...       ...              ...     ...       ...                  ...                ...                 ...          ...\n",
       "                  ...    ...          ...       ...              ...     ...       ...                  ...                ...                 ...          ...\n",
       "Dask Name: merge, 21 tasks"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_merged_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "drawn-excitement",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Map of Ratio of Volume/Area of Catchments"
   ]
  },
  {
   "cell_type": "raw",
   "id": "native-architect",
   "metadata": {},
   "source": [
    "# Large Lake Drainage Area Map:\n",
    "output_map = np.zeros(DB_label.shape)\n",
    "for index,row in DB_merged_prop.iterrows():\n",
    "    [x,y] = np.unravel_index(row.coords_reformatted, DB_label.shape)\n",
    "    output_map[x,y] = row.volume_to_area_ratio"
   ]
  },
  {
   "cell_type": "raw",
   "id": "taken-vehicle",
   "metadata": {},
   "source": [
    "if ice_shelf=='Baudouin': #Baudouin is upside down\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)\n",
    "if ice_shelf=='Nivl': #Nivl is upside down\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "impossible-catholic",
   "metadata": {},
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "handy-petersburg",
   "metadata": {},
   "source": [
    "large_lake_drainage = processed.P_all\n",
    "\n",
    "large_lake_drainage.data = output_map;\n",
    "large_lake_drainage.name = 'large_lake_drainage_area'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "billion-examination",
   "metadata": {},
   "source": [
    "large_lake_drainage.to_netcdf(ice_shelf+'_drainage_volume_to_area.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-spain",
   "metadata": {},
   "source": [
    "### Calculate ratio of volume to area over each drainage area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "apart-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_lake_drainage_area = DB_merged_prop.groupby(DB_merged_prop.large_lake_drainage).Area_in_m.sum()\n",
    "large_lake_drainage_area = large_lake_drainage_area.reset_index()\n",
    "\n",
    "\n",
    "large_lake_drainage_volume = DB_merged_prop.groupby(DB_merged_prop.large_lake_drainage).catchment_volume.sum()\n",
    "\n",
    "large_lake_drainage_volume = large_lake_drainage_volume.reset_index()\n",
    "\n",
    "\n",
    "large_lake_drainage_maxdepth = DB_merged_prop.groupby(DB_merged_prop.large_lake_drainage).max_depth.max()\n",
    "\n",
    "large_lake_drainage_maxdepth= large_lake_drainage_maxdepth.reset_index()\n",
    "\n",
    "\n",
    "\n",
    "large_lake_drainage_lakepercent = DB_merged_prop.groupby(DB_merged_prop.large_lake_drainage).lake_percent.mean()\n",
    "\n",
    "large_lake_drainage_lakepercent = large_lake_drainage_lakepercent.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "amino-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_lake_drainage_area_coords = DB_merged_prop.groupby(DB_merged_prop.large_lake_drainage).coords_reformatted.aggregate([list]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "gross-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "polyphonic-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "large_lake_drainage_area_coords_list = {}\n",
    "for index, row in large_lake_drainage_area_coords.iterrows():\n",
    "    large_lake_drainage_area_coords_list[row.large_lake_drainage] = list(itertools.chain(*row.list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "chicken-degree",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_lake_drainage_area_coords = pd.Series(large_lake_drainage_area_coords_list,name='coordinates').sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "through-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lake = large_lake_drainage_area.merge(large_lake_drainage_volume).merge(large_lake_drainage_maxdepth).merge(large_lake_drainage_lakepercent).set_index(large_lake_drainage_area.large_lake_drainage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "military-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lake = Lake.assign(ratio = lambda x: x['catchment_volume']/x['Area_in_m'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "sunrise-regular",
   "metadata": {},
   "source": [
    "plt.scatter(x=np.log10(Lake.Area_in_m.compute()), y = np.log10(Lake.max_depth.compute()))\n",
    "plt.title(ice_shelf)\n",
    "plt.ylabel('Log10(Max Depth) (m)')\n",
    "plt.xlabel('Area (m^2)')\n",
    "\n",
    "plt.savefig('Pangeo_Exports/maxdepth/'+ice_shelf+'_max_depth.png')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "vulnerable-couple",
   "metadata": {},
   "source": [
    "# Large Lake Drainage Lake Percent Map:\n",
    "output_map = np.zeros(DB_label.shape)\n",
    "for index,item in Lake.ratio.iteritems():\n",
    "    [x,y] = np.unravel_index(large_lake_drainage_area_coords[index], DB_label.shape)\n",
    "    output_map[x,y] = item\n",
    "    \n",
    "if ice_shelf=='Baudouin': #Baudouin is upside down\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)\n",
    "if ice_shelf=='Nivl': #Nivlisen is upside down\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "nonprofit-interim",
   "metadata": {},
   "source": [
    "large_lake_drainage_lakepercent_xr = processed.P_all\n",
    "\n",
    "large_lake_drainage_lakepercent_xr.data = output_map;\n",
    "large_lake_drainage_lakepercent_xr.name = 'volume_to_area_ratio'\n",
    "large_lake_drainage_lakepercent_xr"
   ]
  },
  {
   "cell_type": "raw",
   "id": "trying-kernel",
   "metadata": {},
   "source": [
    "large_lake_drainage_lakepercent_xr[:,11:].coarsen(y=10).mean().plot()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "handy-washington",
   "metadata": {},
   "source": [
    "large_lake_drainage_lakepercent_xr.to_netcdf(str(ice_shelf+'_all_lake_area_to_volume.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-welsh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-translator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-progressive",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "charming-quantum",
   "metadata": {},
   "source": [
    "# Large Lake Drainage Area Map:\n",
    "output_map = np.zeros(DB_label.shape)\n",
    "for index,item in Lake.ratio.iteritems():\n",
    "    [x,y] = np.unravel_index(large_lake_drainage_area_coords[index], DB_label.shape)\n",
    "    output_map[x,y] = item\n",
    "    \n",
    "if ice_shelf=='Baudouin': #Baudouin is upside down\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)\n",
    "if ice_shelf=='Nivl': #Nivlisen is upside down\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "quick-chambers",
   "metadata": {},
   "source": [
    "large_lake_drainage_volumetoarea_xr = processed.P_all\n",
    "\n",
    "large_lake_drainage_volumetoarea_xr.data = output_map;\n",
    "large_lake_drainage_volumetoarea_xr.name = 'large_lake_drainage_volume_to_area'\n",
    "large_lake_drainage_volumetoarea_xr"
   ]
  },
  {
   "cell_type": "raw",
   "id": "finite-development",
   "metadata": {},
   "source": [
    "large_lake_drainage_volumetoarea_xr.to_netcdf(str(ice_shelf+'_all_lake_drainage_area_to_volume_5pix.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-neighborhood",
   "metadata": {},
   "source": [
    " # Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "educational-cinema",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop4=pd.read_csv(ice_shelf+'_MEASUReS_velocity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "sweet-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop4 = DB_prop4.groupby('label').agg({'mean_velocity':'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "technological-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop4 =  DB_prop4.sort_values('label').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "indirect-potato",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>area</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>catchment_volume</th>\n",
       "      <th>coords</th>\n",
       "      <th>Area_in_m</th>\n",
       "      <th>volume_to_area_ratio</th>\n",
       "      <th>coords_reformatted</th>\n",
       "      <th>large_lake_drainage</th>\n",
       "      <th>lake_percent</th>\n",
       "      <th>index</th>\n",
       "      <th>mean_velocity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=10</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>object</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: merge, 32 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                label   area eccentricity max_depth catchment_volume  coords Area_in_m volume_to_area_ratio coords_reformatted large_lake_drainage lake_percent  index mean_velocity\n",
       "npartitions=10                                                                                                                                                                      \n",
       "                int64  int64      float64   float64          float64  object   float64              float64             object               int64      float64  int64       float64\n",
       "                  ...    ...          ...       ...              ...     ...       ...                  ...                ...                 ...          ...    ...           ...\n",
       "...               ...    ...          ...       ...              ...     ...       ...                  ...                ...                 ...          ...    ...           ...\n",
       "                  ...    ...          ...       ...              ...     ...       ...                  ...                ...                 ...          ...    ...           ...\n",
       "                  ...    ...          ...       ...              ...     ...       ...                  ...                ...                 ...          ...    ...           ...\n",
       "Dask Name: merge, 32 tasks"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_merged_prop = DB_merged_prop.merge(DB_prop4)\n",
    "DB_merged_prop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "mobile-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop4 = pd.read_csv(ice_shelf+'_MEASUReS_velocity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "knowing-tractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_lake_mean_velocity = DB_merged_prop.groupby(DB_merged_prop.large_lake_drainage).mean_velocity.mean().reset_index().compute()\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "individual-version",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_lake_mean_velocity = by_lake_mean_velocity.set_index(by_lake_mean_velocity.large_lake_drainage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "favorite-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lake = Lake.assign(velocity = by_lake_mean_velocity.mean_velocity)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "organized-renewal",
   "metadata": {},
   "source": [
    "output_map = np.zeros(DB_label.shape)\n",
    "for index,row in Lake.iterrows():\n",
    "    [x,y] = np.unravel_index(large_lake_drainage_area_coords[index], DB_label.shape)\n",
    "    output_map[x,y] = row.velocity\n",
    "\n",
    "if np.isin(ice_shelf,['Baudouin','Nivl']): #Baudouin is upside down\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cognitive-recall",
   "metadata": {},
   "source": [
    "velocity_DB_xr = processed.P_all"
   ]
  },
  {
   "cell_type": "raw",
   "id": "liquid-audio",
   "metadata": {},
   "source": [
    "velocity_DB_xr.data = output_map;\n",
    "velocity_DB_xr.name = 'Mean_Velocity'\n",
    "velocity_DB_xr"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ethical-resistance",
   "metadata": {},
   "source": [
    "velocity_DB_xr.to_netcdf(ice_shelf+'_mean_velocity_by_lake_5pix.nc')\n",
    "#cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-manchester",
   "metadata": {},
   "source": [
    "# Subshelf Melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "rural-sixth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LarsenC'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ice_shelf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "collectible-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop5 = pd.read_csv(ice_shelf+'_subshelf_melt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "floppy-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop5 =  DB_prop5.groupby('label').subshelfmelt.sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "pharmaceutical-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop5 = DB_prop5.set_index(DB_prop5.label)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "polyphonic-design",
   "metadata": {},
   "source": [
    "DB_prop5.to_csv(ice_shelf+'_subshelf_melt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "electoral-fetish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>subshelfmelt</th>\n",
       "      <th>large_lake_drainage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277237.0</th>\n",
       "      <td>277237.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277238.0</th>\n",
       "      <td>277238.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277239.0</th>\n",
       "      <td>277239.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277240.0</th>\n",
       "      <td>277240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277241.0</th>\n",
       "      <td>277241.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>277221 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             label  subshelfmelt  large_lake_drainage\n",
       "label                                                \n",
       "1.0            1.0           0.0                    0\n",
       "3.0            3.0           0.0                49106\n",
       "4.0            4.0           0.0                49106\n",
       "5.0            5.0           0.0                48810\n",
       "6.0            6.0           0.0                49106\n",
       "...            ...           ...                  ...\n",
       "277237.0  277237.0           0.0                    0\n",
       "277238.0  277238.0           0.0                    0\n",
       "277239.0  277239.0           0.0                    0\n",
       "277240.0  277240.0           0.0                    0\n",
       "277241.0  277241.0           0.0                    0\n",
       "\n",
       "[277221 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB_prop5['large_lake_drainage'] = DB_merged_prop.large_lake_drainage\n",
    "DB_prop5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "upper-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_lake_subshelf_melt = DB_prop5.groupby(DB_prop5.large_lake_drainage).subshelfmelt.sum().reset_index()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "metropolitan-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_lake_subshelf_melt = by_lake_subshelf_melt.set_index(by_lake_subshelf_melt.large_lake_drainage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "sorted-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lake = Lake.assign(subshelf_melt = by_lake_subshelf_melt.subshelfmelt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "pediatric-compromise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>large_lake_drainage</th>\n",
       "      <th>Area_in_m</th>\n",
       "      <th>catchment_volume</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>lake_percent</th>\n",
       "      <th>ratio</th>\n",
       "      <th>velocity</th>\n",
       "      <th>subshelf_melt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63083</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: assign, 105 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "              large_lake_drainage Area_in_m catchment_volume max_depth lake_percent    ratio velocity subshelf_melt\n",
       "npartitions=1                                                                                                      \n",
       "0                           int64   float64          float64   float64      float64  float64  float64       float64\n",
       "63083                         ...       ...              ...       ...          ...      ...      ...           ...\n",
       "Dask Name: assign, 105 tasks"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lake"
   ]
  },
  {
   "cell_type": "raw",
   "id": "minimal-television",
   "metadata": {},
   "source": [
    "output_map = np.zeros(DB_label.shape)\n",
    "for index,row in Lake.iterrows():\n",
    "    [x,y] = np.unravel_index(large_lake_drainage_area_coords[index], DB_label.shape)\n",
    "    output_map[x,y] = row.subshelf_melt\n",
    "\n",
    "if np.isin(ice_shelf,['Baudouin','Nivl']): #Baudouin is upside down\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "beneficial-civilization",
   "metadata": {},
   "source": [
    "subshelf_melt_DB_xr = processed.P_all"
   ]
  },
  {
   "cell_type": "raw",
   "id": "wrapped-arrival",
   "metadata": {},
   "source": [
    "subshelf_melt_DB_xr.data = output_map;\n",
    "subshelf_melt_DB_xr.name = 'Subshelf_Melt_Rate'\n",
    "subshelf_melt_DB_xr"
   ]
  },
  {
   "cell_type": "raw",
   "id": "located-silicon",
   "metadata": {},
   "source": [
    "subshelf_melt_DB_xr.to_netcdf(ice_shelf+'_subshelf_melt_by_lake_5pix.nc')\n",
    "#cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indonesian-diversity",
   "metadata": {},
   "source": [
    "# Vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "compliant-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop6 = pd.read_csv(ice_shelf+'_ice_shelf_vulnerability.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "collectible-tanzania",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop6 =  DB_prop6.groupby('label').agg({'vulnerability':'max'}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "present-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop6 = DB_prop6.assign(is_floating = np.greater_equal(DB_prop6.vulnerability,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "banner-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop6 = DB_prop6.set_index(DB_prop6.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "later-water",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop6 = DB_prop6.assign(large_lake_drainage = DB_merged_prop.large_lake_drainage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "emotional-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_lake_vulnerability = DB_prop6.groupby(DB_prop6.large_lake_drainage).vulnerability.mean().reset_index()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aboriginal-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_lake_vulnerability = by_lake_vulnerability.set_index(by_lake_vulnerability.large_lake_drainage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ordered-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lake = Lake.assign(vulnerability = by_lake_vulnerability.vulnerability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "radical-nevada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>large_lake_drainage</th>\n",
       "      <th>Area_in_m</th>\n",
       "      <th>catchment_volume</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>lake_percent</th>\n",
       "      <th>ratio</th>\n",
       "      <th>velocity</th>\n",
       "      <th>subshelf_melt</th>\n",
       "      <th>vulnerability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>int64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63083</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: assign, 107 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "              large_lake_drainage Area_in_m catchment_volume max_depth lake_percent    ratio velocity subshelf_melt vulnerability\n",
       "npartitions=1                                                                                                                    \n",
       "0                           int64   float64          float64   float64      float64  float64  float64       float64       float64\n",
       "63083                         ...       ...              ...       ...          ...      ...      ...           ...           ...\n",
       "Dask Name: assign, 107 tasks"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lake"
   ]
  },
  {
   "cell_type": "raw",
   "id": "sought-format",
   "metadata": {},
   "source": [
    "output_map = np.zeros(DB_label.shape)\n",
    "for index,row in Lake.iterrows():\n",
    "    [x,y] = np.unravel_index(large_lake_drainage_area_coords[index], DB_label.shape)\n",
    "    output_map[x,y] = row.vulnerability\n",
    "\n",
    "if np.isin(ice_shelf,['Baudouin','Nivl']): #Baudouin is upside down\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "divided-portfolio",
   "metadata": {},
   "source": [
    "vulnerability_DB_xr = processed.P_all"
   ]
  },
  {
   "cell_type": "raw",
   "id": "clinical-aruba",
   "metadata": {},
   "source": [
    "vulnerability_DB_xr.data = output_map;\n",
    "vulnerability_DB_xr.name = 'vulnerability'\n",
    "vulnerability_DB_xr"
   ]
  },
  {
   "cell_type": "raw",
   "id": "contemporary-single",
   "metadata": {},
   "source": [
    "vulnerability_DB_xr.to_netcdf(ice_shelf+'_vulnerability_by_lake_5pix.nc')\n",
    "#cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-satin",
   "metadata": {},
   "source": [
    "# SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "becoming-ghost",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop8 = pd.read_csv(ice_shelf+'_RACMO_SM.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "solar-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop8 = DB_prop8.groupby('label').agg({'mean_SM':'mean',\n",
    "                                         'area':'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "tested-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop8 = DB_prop8.assign(annual_SM_in_m= DB_prop8.mean_SM)#(32**2)*DB_prop8.area*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "reduced-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop8 = DB_prop8.assign(annual_SM_in_m_volume= DB_prop8.annual_SM_in_m*(32**2)*DB_prop8.area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "certified-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop8 = DB_prop8.set_index(DB_prop8.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fourth-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_prop8 = DB_prop8.assign(large_lake_drainage = DB_merged_prop.large_lake_drainage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "express-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_lake_SM = DB_prop8.groupby(DB_prop8.large_lake_drainage).agg({'annual_SM_in_m_volume':'sum','area':'sum'}).reset_index()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "controlling-publisher",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_lake_SM = by_lake_SM.set_index(by_lake_SM.large_lake_drainage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "front-uncertainty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>large_lake_drainage</th>\n",
       "      <th>annual_SM_in_m_volume</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>large_lake_drainage</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.654500e+09</td>\n",
       "      <td>12582234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.198093e+05</td>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.428461e+05</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.838491e+05</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5.715381e+05</td>\n",
       "      <td>1744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63079</th>\n",
       "      <td>63079</td>\n",
       "      <td>2.992783e+00</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63080</th>\n",
       "      <td>63080</td>\n",
       "      <td>7.375074e+00</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63081</th>\n",
       "      <td>63081</td>\n",
       "      <td>1.726495e+04</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63082</th>\n",
       "      <td>63082</td>\n",
       "      <td>1.859709e+03</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63083</th>\n",
       "      <td>63083</td>\n",
       "      <td>2.084157e+02</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62816 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     large_lake_drainage  annual_SM_in_m_volume      area\n",
       "large_lake_drainage                                                      \n",
       "0                                      0           1.654500e+09  12582234\n",
       "1                                      1           2.198093e+05       687\n",
       "2                                      2           2.428461e+05       759\n",
       "3                                      3           1.838491e+05       561\n",
       "4                                      4           5.715381e+05      1744\n",
       "...                                  ...                    ...       ...\n",
       "63079                              63079           2.992783e+00        84\n",
       "63080                              63080           7.375074e+00       207\n",
       "63081                              63081           1.726495e+04       100\n",
       "63082                              63082           1.859709e+03       232\n",
       "63083                              63083           2.084157e+02        26\n",
       "\n",
       "[62816 rows x 3 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_lake_SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ranking-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lake = Lake.assign(SM = by_lake_SM.annual_SM_in_m_volume/(by_lake_SM.area*32**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "recreational-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lake.compute().to_csv(ice_shelf+'_props_by_lake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "august-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-problem",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-relaxation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "parliamentary-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lake = pd.read_csv(ice_shelf+'_props_by_lake.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "suitable-trunk",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "146",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 146",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-8e951d1549c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDB_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlarge_lake_drainage_area_coords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDB_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0moutput_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 146"
     ]
    }
   ],
   "source": [
    "output_map = np.zeros(DB_label.shape)\n",
    "for index,row in Lake.iterrows():\n",
    "    [x,y] = np.unravel_index(large_lake_drainage_area_coords[index], DB_label.shape)\n",
    "    output_map[x,y] = row.SM\n",
    "\n",
    "if np.isin(ice_shelf,['Baudouin','Nivl']): #Baudouin is upside down\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(output_map)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "tracked-jenny",
   "metadata": {},
   "source": [
    "if ice_shelf=='Baudouin': #Baudouin is upside down\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)\n",
    "if ice_shelf=='Nivl': #Nivl is upside down\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)\n",
    "    output_map = np.fliplr(output_map)\n",
    "    output_map = np.flipud(output_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_lake_drainage = processed.P_all\n",
    "\n",
    "large_lake_drainage.data = output_map;\n",
    "large_lake_drainage.name = 'annual_average_surface_melt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-isaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_lake_drainage.to_netcdf(ice_shelf+'_surface_melt_by_lake_m.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-picture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "warming-mercury",
   "metadata": {},
   "source": [
    "## Comparing to Future Melt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "rotary-badge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "occasional-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = gateway.new_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "advisory-lindsay",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "southern-texture",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-seven",
   "metadata": {},
   "source": [
    "### Decadal Avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "biblical-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['ACCESS1.3','CESM2','CNRM-CM6','NorESM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "absolute-memphis",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-814686357faf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCMIP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Kittel_futureSMB/year-MAR_NorESM-1980-2100_zen.nc2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrhow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mCMIP_RU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCMIP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mufuncs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCMIP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m9999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mrhow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xr' is not defined"
     ]
    }
   ],
   "source": [
    "CMIP = xr.open_dataset('Kittel_futureSMB/year-MAR_NorESM-1980-2100_zen.nc2',)\n",
    "rhow = 1000\n",
    "CMIP_RU = CMIP.RU.where(xr.ufuncs.isfinite(CMIP.RU),-9999)/rhow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dimensional-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "CMIP_RU =CMIP_RU.assign_coords({'X':CMIP_RU.X*1000, 'Y':CMIP_RU.Y*1000}).rio.write_crs('epsg:3031')\n",
    "\n",
    "CMIP_RU = CMIP_RU.rio.set_spatial_dims('X','Y').rename({'X':'x','Y':'y'})\n",
    "\n",
    "CMIP_RU_decadal = CMIP_RU.groupby(np.floor(CMIP_RU.TIME.dt.year/5)).max().squeeze()\n",
    "\n",
    "CMIP_RU_decadal = CMIP_RU_decadal[9:,:,:] #After 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_map = xr.open_dataarray(ice_shelf + '_all_lake_drainage_area_to_volume.nc')\n",
    "\n",
    "x1,y1,x2,y2 = processed.rio.bounds()\n",
    "maxx = max(x1,x2)\n",
    "minx = min(x1,x2)\n",
    "maxy = max(y1, y2)\n",
    "miny = min(y1,y2)\n",
    "if ice_shelf=='Baudouin' or ice_shelf=='Nivl': \n",
    "    CMIP_RU_decadal_clipped = CMIP_RU_decadal.rio.clip_box(minx,miny,maxx,maxy)\n",
    "else:\n",
    "    CMIP_RU_decadal_clipped = CMIP_RU_decadal.rio.clip_box(miny,minx,maxy,maxx)\n",
    "\n",
    "Y = processed.y.values\n",
    "X = processed.x.values\n",
    "mesh_X, mesh_Y = np.meshgrid(X,Y)\n",
    "#if name=='Baudouin' or name=='Nivl': \n",
    " #   coords = [mesh_X.ravel(),mesh_Y.ravel()]\n",
    "#else:\n",
    "coords = [mesh_Y.ravel(),mesh_X.ravel()]\n",
    "\n",
    "CMIP_X,CMIP_Y = np.meshgrid(CMIP_RU_decadal_clipped.x, CMIP_RU_decadal_clipped.y)\n",
    "\n",
    "points = np.transpose([CMIP_Y.ravel(), CMIP_X.ravel()])\n",
    "\n",
    "xi = np.transpose(coords)\n",
    "years = CMIP_RU_decadal_clipped.year.values*5\n",
    "whenlarger = np.zeros(islake_labelled.data.shape)\n",
    "\n",
    "for t in np.arange(0, len(CMIP_RU_decadal_clipped.year)):\n",
    "    #if name=='Baudouin' or name=='Nivl':\n",
    "     #   data = CESM_RU_decadal_clipped[t,:,:].transpose().data.ravel()\n",
    "    #else:\n",
    "    data = CMIP_RU_decadal_clipped[t,:,:].data.transpose().ravel()\n",
    "\n",
    "    CMIP_interp = scipy.interpolate.griddata(points,data,xi,method='nearest')\n",
    "\n",
    "    CMIP_interp = CMIP_interp.reshape([ratio_map.shape[0],ratio_map.shape[1]])\n",
    "\n",
    "    mask = whenlarger==0\n",
    "\n",
    "    whenlarger[np.bitwise_and(CMIP_interp>ratio_map, mask)] = years[t]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "positive-treat",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ae0544d5d84b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhenlarger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(whenlarger)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(CMIP_RU_decadal_clipped.sum(dim='year').transpose(),vmin=0, vmax=1)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_map[5:,5:].coarsen(x=10,y=10).mean().transpose().plot(vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-hierarchy",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "counter = 1\n",
    "for index, row in IS_processed.iterrows():\n",
    "\n",
    "    if row.NAME=='Riiser-Larsen':\n",
    "        name = 'Riiser'\n",
    "    else:\n",
    "        name = row.NAME\n",
    "\n",
    "    ratio_map = xr.open_dataarray(name + '_all_lake_drainage_area_to_volume.nc')\n",
    "\n",
    "    x1,y1,x2,y2 = ratio_map.rio.bounds()\n",
    "    maxx = max(x1,x2)\n",
    "    minx = min(x1,x2)\n",
    "    maxy = max(y1, y2)\n",
    "    miny = min(y1,y2)\n",
    "    if name=='Baudouin' or name=='Nivl': \n",
    "        CESM_RU_decadal_clipped = CESM_RU_decadal.rio.clip_box(minx,miny,maxx,maxy)\n",
    "    else:\n",
    "        CESM_RU_decadal_clipped = CESM_RU_decadal.rio.clip_box(miny,minx,maxy,maxx)\n",
    "\n",
    "    Y = ratio_map.y.values\n",
    "    X = ratio_map.x.values\n",
    "    mesh_X, mesh_Y = np.meshgrid(X,Y)\n",
    "    if name=='Baudouin' or name=='Nivl': \n",
    "        coords = [mesh_X.ravel(),mesh_Y.ravel()]\n",
    "    else:\n",
    "        coords = [mesh_Y.ravel(),mesh_X.ravel()]\n",
    "\n",
    "\n",
    "    CESM_X,CESM_Y = np.meshgrid(CESM_RU_decadal_clipped.x, CESM_RU_decadal_clipped.y)\n",
    "\n",
    "    points = np.transpose([CESM_X.ravel(), CESM_Y.ravel()])\n",
    "\n",
    "    xi = np.transpose(coords)\n",
    "    years = CESM_RU_decadal_clipped.year.values\n",
    "    whenlarger = np.zeros(ratio_map.data.shape)\n",
    "    for t in np.arange(0, len(CESM_RU_decadal_clipped.year)):\n",
    "        if name=='Baudouin' or name=='Nivl':\n",
    "            data = CESM_RU_decadal_clipped[t,:,:].transpose().data.ravel()\n",
    "        else:\n",
    "            data = CESM_RU_decadal_clipped[t,:,:].data.ravel()\n",
    "\n",
    "        CMIP_interp = scipy.interpolate.griddata(points,data,xi,method='nearest')\n",
    "\n",
    "        CMIP_interp = CMIP_interp.reshape([ratio_map.shape[0],ratio_map.shape[1]])\n",
    "        \n",
    "        mask = whenlarger==0\n",
    "        \n",
    "        whenlarger[np.bitwise_and(CMIP_interp>ratio_map, mask)] = years[t]\n",
    "        \n",
    "\n",
    "    #decade_list = CESM_RU_decadal_clipped.year.values*10\n",
    "    #year_list = CESM_RU_accum.TIME.dt.year\n",
    "\n",
    "    #larger_decade=np.zeros(ratio_interp_ravelled.shape)\n",
    "    #for x in np.arange(0, len(ratio_interp_ravelled)):\n",
    "     #   if ratio_interp_ravelled[x]!=0:\n",
    "      #      if np.any(ratio_interp_ravelled[x]<CESM_ravelled[x,:]):      \n",
    "       #         larger_decade[x] = decade_list[np.argwhere(ratio_interp_ravelled[x]<CESM_ravelled[x,:])[0]]\n",
    "        #    else:\n",
    "         #       larger_decade[x] = np.nan\n",
    "        #else:\n",
    "         #   larger_decade[x] = 0\n",
    "\n",
    "    #larger_decade = larger_decade.reshape([CESM_RU_decadal_clipped.shape[1],CESM_RU_decadal_clipped.shape[2]])\n",
    "   \n",
    "    ax = fig.add_subplot(3,2, counter)\n",
    "    ax.set_title(name)\n",
    "    cmap = mpl.cm.inferno\n",
    "    #cmap.set_bad('black',1.)\n",
    "    #larger_decade = np.ma.array(larger_decade, mask=np.isnan(larger_decade))\n",
    "    if name=='Baudouin' or name=='Nivl':\n",
    "        h=ax.pcolormesh(Y,X,whenlarger, shading='auto',vmin=2010,vmax=2100, cmap=cmap)\n",
    "    else:\n",
    "        h=ax.pcolormesh(X,Y,whenlarger, shading='auto',vmin=2010,vmax=2100, cmap=cmap)\n",
    "\n",
    "\n",
    "\n",
    "    plt.colorbar(h)\n",
    "    IS_processed[IS_processed.NAME==row.NAME].geometry.plot(ax=ax,facecolor='None', edgecolor='b')\n",
    "    counter = counter+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-breeding",
   "metadata": {},
   "source": [
    "### Accumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNRM = xr.open_dataset('Kittel_futureSMB/year-MAR_NorESM-1980-2100_zen.nc2')\n",
    "rhow = 1000\n",
    "CNRM_RU = CNRM.RU.where(xr.ufuncs.isfinite(CNRM.RU),-9999)/rhow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNRM_RU =CNRM_RU.assign_coords({'X':CNRM_RU.X*1000, 'Y':CNRM_RU.Y*1000}).rio.write_crs('epsg:3031')\n",
    "\n",
    "CNRM_RU = CNRM_RU.rio.set_spatial_dims('X','Y').rename({'X':'x','Y':'y'})\n",
    "\n",
    "CNRM_RU_decadal = CNRM_RU.groupby(np.floor(CNRM_RU.TIME.dt.year/10)).mean().squeeze()\n",
    "\n",
    "CNRM_RU_decadal = CNRM_RU_decadal[4:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNRM_RU_accum = CNRM_RU[CNRM_RU.TIME.dt.year>=2015,0,:,:].cumsum(dim='TIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "counter = 1\n",
    "for index, row in IS_processed.iterrows():\n",
    "\n",
    "    if row.NAME=='Riiser-Larsen':\n",
    "        name = 'Riiser'\n",
    "    else:\n",
    "        name = row.NAME\n",
    "\n",
    "    ratio_map = xr.open_dataarray(name + '_all_lake_drainage_area_to_volume.nc')\n",
    "\n",
    "    x1,y1,x2,y2 = ratio_map.rio.bounds()\n",
    "    maxx = max(x1,x2)\n",
    "    minx = min(x1,x2)\n",
    "    maxy = max(y1, y2)\n",
    "    miny = min(y1,y2)\n",
    "    if name=='Baudouin' or name=='Nivl': \n",
    "        CNRM_RU_accum_clipped = CNRM_RU_accum.rio.clip_box(minx,miny,maxx,maxy)\n",
    "    else:\n",
    "        CNRM_RU_accum_clipped = CNRM_RU_accum.rio.clip_box(miny,minx,maxy,maxx)\n",
    "\n",
    "    Y = ratio_map.y.values\n",
    "    X = ratio_map.x.values\n",
    "    mesh_X, mesh_Y = np.meshgrid(X,Y)\n",
    "    if name=='Baudouin' or name=='Nivl': \n",
    "        coords = [mesh_X.ravel(),mesh_Y.ravel()]\n",
    "    else:\n",
    "        coords = [mesh_Y.ravel(),mesh_X.ravel()]\n",
    "\n",
    "\n",
    "    CNRM_X,CNRM_Y = np.meshgrid(CNRM_RU_accum_clipped.x, CNRM_RU_accum_clipped.y)\n",
    "\n",
    "    xi = np.transpose([CNRM_X.ravel(), CNRM_Y.ravel()])\n",
    "\n",
    "    points = np.transpose(coords)\n",
    "    if name=='Baudouin' or name=='Nivl':\n",
    "        data = ratio_map.transpose().data.ravel()\n",
    "    else:\n",
    "        data = ratio_map.data.ravel()\n",
    "\n",
    "    ratio_interp = scipy.interpolate.griddata(points,data,xi,method='nearest')\n",
    "\n",
    "    ratio_interp = ratio_interp.reshape([CNRM_RU_accum_clipped.shape[1],CNRM_RU_accum_clipped.shape[2]])\n",
    "\n",
    "    ratio_interp_xr = xr.DataArray(ratio_interp, (CNRM_RU_accum_clipped.y,CNRM_RU_accum_clipped.x))\n",
    "    CNRM_ravelled = CNRM_RU_accum_clipped.data.reshape([CNRM_RU_accum_clipped.shape[1]* CNRM_RU_accum_clipped.shape[2],CNRM_RU_accum_clipped.shape[0]])\n",
    "    ratio_interp_ravelled = ratio_interp_xr.data.ravel()\n",
    "\n",
    "    #decade_list = CESM_RU_decadal_clipped.year.values*10\n",
    "    year_list = CNRM_RU_accum.TIME.dt.year\n",
    "\n",
    "    larger_decade=np.zeros(ratio_interp_ravelled.shape)\n",
    "    for x in np.arange(0, len(ratio_interp_ravelled)):\n",
    "        if ratio_interp_ravelled[x]!=0:\n",
    "            if np.any(ratio_interp_ravelled[x]<CNRM_ravelled[x,:]):      \n",
    "                larger_decade[x] = year_list[np.argwhere(ratio_interp_ravelled[x]<CNRM_ravelled[x,:])[0]]\n",
    "            else:\n",
    "                larger_decade[x] = np.nan\n",
    "        else:\n",
    "            larger_decade[x] = 0\n",
    "\n",
    "    larger_decade = larger_decade.reshape([CNRM_RU_accum_clipped.shape[1],CNRM_RU_accum_clipped.shape[2]])\n",
    "   \n",
    "    ax = fig.add_subplot(3,2, counter)\n",
    "    ax.set_title(name)\n",
    "    cmap = mpl.cm.tab20b\n",
    "    cmap.set_bad('black',1.)\n",
    "    larger_decade = np.ma.array(larger_decade, mask=np.isnan(larger_decade))\n",
    "\n",
    "    h=ax.pcolormesh(CNRM_RU_accum_clipped.x,CNRM_RU_accum_clipped.y,larger_decade, shading='auto',vmin=2000,vmax=2100, cmap=cmap)\n",
    "\n",
    "\n",
    "    plt.colorbar(h)\n",
    "    IS_processed[IS_processed.NAME==row.NAME].geometry.plot(ax=ax,facecolor='None', edgecolor='b')\n",
    "    counter = counter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-superior",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
